Torch中[{{begin, end}}]是头尾包含的

调用网络forward的时候一定要检查是否需要cuda()
碰到类无法识别的情况可能是参数多传了一个
image库需要确保穿进去的图片不是CudaTensor
backward参数是两个：（输入，对应输出的gradient)

(Baseline_N)从第一层relevance用3层convolution回归，loss只能到0.33。检查发现是要么全负要么全正，在人眼看来深浅可以近似表达出物体的轮廓。也许一个聪明一点的阈值二值化就可以了？在神经网络上可以用1*1 convolution代替，但是这样再加一层卷积就可以了吗？还没有试过。

(Baseline_Naive)为了排除是训练难以收敛（其实是不太可能的，因为是拟合一个(3,224,224) -> (3,224,224)的函数，而且只有三层），我就训练了一个convolution层。结果loss是0.37怎么样也降不下来。查看图片发现并没有什么合理性：从relevance到mask还不是那么显然的。
或者说这个生成式的模型不太适合卷积作？Sharpmask中也没有采用从小mask直接refine到大mask的策略，而是采用mask encoding的说法,从上到下refine的是一个多层的feature，直到最后一层才变成单层的mask。

按照我的直觉，relevance从形态上是比较接近mask的，那么单层理论上就应该达到比较好的效果，再退一步，多层的conv也应该足够出一些有意义的了，但是并不是这样的。原因可能有：1.这个relevance有问题。2.优化有问题。
1的解决方案是：
（1）换一种relevance。但是各个relevance差别并没有那么大，这属于调优的范围了。
（2）输入空间上的relevance并不具有mask的潜力。从直觉上看，这种方法是有说服力的：输入空间上对干扰没有稳定性。如果一个输入空间的像素点即便剧烈的变换，那么他对于分类的贡献也应该是不变的，但是这个像素点却会有很醒目的relevance变化。
    那么解决办法是：换高层。直接看高层relevance的结果，看有没有可能。这样就涉及到高维空间可视化的问题了，这里我们可以简化：从高位feature上看像素贡献值，无非也就是要将多维向量用一个点表示出来；如果高位feature数值上越高，那么可以认为这个点的贡献值也越高，可以采用简单加和的表示方法。 
    经过观察我发现第六层resnet unit的gradInput加和后具有较好的可视化特性。对于不同的图片4～7层都可能具有很好的texing。
2的解决办法是：
（1）如果是怀疑CNN不具有这种能力的话，可以参考生成式模型中的实例设计网络结构。
（2）或许使用和deepmask一样的结构会更好。因为我此处使用的是deepmask的sampler，数据上是设计成为maskBranch和scorebranch的。

实际上实施的结果：
1.(2)从多个层开始，使用